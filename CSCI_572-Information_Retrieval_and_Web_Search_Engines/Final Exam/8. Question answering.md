# Question answering

### Information Retrieval v. Question Answering

- The name **“information retrieval”** is standard, but as traditionally practiced, it’s not really right
- In the past all we got was **document retrieval**, and beyond that the job is up to us
  - Modern search engines now do a lot better
- They combine information retrieval, knowledge graphs and inferencing, past query history, location, natural language processing and many other criteria
- **Question Answering (QA)** is concerned with building systems that automatically answer questions posed by humans in a natural language

### People want to ask questions…

**Examples from Ask.com query log**
how much should I weigh
what does my name mean
how to get pregnant
where can I find pictures of hairstyles
who is the richest man in the world
what is the meaning of life
why is the sky blue
what is the difference between white eggs and brown eggs
can you drink milk after the expiration date
what is true love
what is the jonas brothers address

**Around 10-20% of query logs are questions such as these**

### Question: Who was the prime minister of Australia during the Great Depression? Answer: James Scullin (Labor) 1929-31

<img src="8. Question answering.assets/image-20250505204928183.png" alt="image-20250505204928183" style="zoom:80%;" />

##### Google's result today

<img src="8. Question answering.assets/image-20250505204959347.png" alt="image-20250505204959347" style="zoom:80%;" />

### Google has Improved Its Ability to Answer Many Questions

<img src="8. Question answering.assets/image-20250505205024051.png" alt="image-20250505205024051" style="zoom:80%;" />

### Some Questions are Easily Answered

<img src="8. Question answering.assets/image-20250505205046124.png" alt="image-20250505205046124" style="zoom:80%;" />



### **The Original Google Approach**

- Take the question and try to find it as a string on the web
- Return the next sentence on that web page as the answer
- Works brilliantly if this exact question appears as a FAQ question, etc.
- Works poorly most of the time
- But a more sophisticated version of this approach has been introduced in recent years combining
  - Knowledge graph
  - N-grams
  - WordNet
  - NLP techniques



### **Many Questions Pose Semantic Difficulties**

<img src="8. Question answering.assets/image-20250505205144358.png" alt="image-20250505205144358" style="zoom:80%;" />

- Who is Michael Jordan?
  - Michael Jordan the basketball player or the Machine Learning guy?
  - <img src="8. Question answering.assets/image-20250505205151721.png" alt="image-20250505205151721" style="zoom:80%;" />
- Key requirement is that entities get identified and disambiguated

### **Why Natural Language Processing is Required**

- **Question:** “When was Wendy’s founded?”
- **Passage candidate:**
   – “The renowned Murano glassmaking industry, on an island in the Venetian lagoon, has gone through several reincarnations since it was **founded** in 1291. Three exhibitions of **20th-century** Murano glass are coming up in New York. By **Wendy** Moonan.”
- **Answer:** 20th Century
- *the candidate passage below includes the relevant keywords (Wendy’s, founded), but provides an incorrect answer*

### **More NLP Challenges - Predicate-Argument Structure**

- **Q336:** *When was Microsoft established?*

- Difficult because Microsoft tends to establish lots of things…

  *Microsoft plans to establish manufacturing partnerships in Brazil and Mexico in May.*

- Need to be able to detect sentences in which ‘Microsoft’ is object of ‘establish’ or close synonym.

- A correct result might *not* even include the query term:

  *Microsoft Corp was founded in the US in 1975, incorporated in 1981, and established in the UK in 1982.*

*NLP: Natural language processing*



### **Some Questions Require Inferences**

**What is the distance between the largest city in California and the largest city in Nevada?**

Google does poorly on this query, misinterpreting Nevada as Nevada County, California

ps: Try the query in Google today to see if they have improved their answer

<img src="8. Question answering.assets/image-20250505205331386.png" alt="image-20250505205331386" style="zoom:80%;" />

### **In Some Cases the Data May Not Exist**

**how many Ph.D. degrees in mathematics were granted by European universities in 1986?**

All results are irrelevant →

<img src="8. Question answering.assets/image-20250505205707967.png" alt="image-20250505205707967" style="zoom:80%;" />

a more recent result; still no relevant links

<img src="8. Question answering.assets/image-20250505205717997.png" alt="image-20250505205717997" style="zoom:80%;" />



### **Some Popular Products Designed for Question/Answering**

- **Approach 1: used by Siri**
  - map to known entities and use existing databases over the internet
- **Approach 2: used by Ask.com**
  - detect question type and use a search engine’s snippet results
- **Approach 3: used by IBM’s Watson**
  - combine approaches 1 and 2
- **Approach 4: used by Google’s Knowledge Graph**
  - use an entity - relationship graph that is built in-house and infer the answer

### **Approach used by Siri: Knowledge-Based Approach**

- **Siri was begun as a DARPA project called CALO/PAL (Personalized Assistant that Learns)**

1. First your voice query is put through a recognizer and a language model and Siri comes up with an interpretation of what was said
2. Second Siri builds a semantic representation of the query
   - Extract times, dates, locations, entities, numeric quantities, and especially user actions (e.g. schedule a meeting, set my alarm, etc)
3. Siri maps from this semantics to query structured data or resources including:
   - Geospatial databases
   - Ontologies (Wikipedia infoboxes, Freebase, WordNet, Yago)
   - Restaurant review sources and reservation services (Yelp)
   - Scientific databases (Wolfram Alpha)
   - Conventional search engines (Google, Bing)
4. Siri then transforms the output above into natural language and then transforms the text back to speech

### Context and Conversation in Virtual Assistants like Siri

- Coreference helps resolve ambiguities

- U : "book a table at Il Fornaio at 7:00 with my mom "
- U : " also send her an email reminder "
- " her " refers to " my mom " ; 7:00 refers to pm , not am Clarification questions :
- U : " chicago pizza "
- S : " Did you mean pizza restaurants in Chicago or Chicago- style pizza ? "

### CANDIDATE ANSWER SCORING IN IBM WATSON

- Each candidate answer gets scores from > 50 components

- From unstructured text , semi - structured text , triple stores

- Logical form (parse) match between question and candidate

- Passage source reliability

- Geospatial location

  - Denver is " southwest of Montana " is a good answer given directionality , e.g. see [geonames.org](http://www.geonames.org/) which gives latitude / longitude / population / etc

- Temporal relationships

  - "In 1594 he took a job as a tax collector in Andalusia"

  - Candidates : Thoreau is a bad answer (born in 1817)

  - Candidates : Cervantes is possible (he was alive in 1594)

- Taxonomic classification



### AskJeeves (now Ask.com)

- **Earlier AskJeeves.com** was well-known as a search engine specializing in Questions/Answers
- Though it still exists, it performs far weaker than sites such as Google

How old is Mariah Carey
 Snapshot taken 04/2022

<img src="8. Question answering.assets/image-20250505210223489.png" alt="image-20250505210223489" style="zoom:80%;" />

How tall is mount Everest
 Snapshot taken 04/2022

<img src="8. Question answering.assets/image-20250505210229785.png" alt="image-20250505210229785" style="zoom:80%;" />



### **3 Main Phases for Question/Answering**

1. **QUESTION PROCESSING**
   - Detect question type (who, what, when, where, etc)
   - Identify important entities and formulate queries to send to a search engine
2. **PASSAGE RETRIEVAL**
   - Retrieve ranked documents (snippets only)
   - Break into suitable passages and match against entities
3. **ANSWER PROCESSING**
   - Extract candidate answers (as named entities)
   - Rank candidates

*using evidence from relations in the text and external sources*

### **Question Answering 3 Phase Block Architecture**

<img src="8. Question answering.assets/image-20250505210351823.png" alt="image-20250505210351823" style="zoom:80%;" />



### **Question Types: Many Questions Fall into Distinct Categories**

| **Who**      | Person, Organization |
| ------------ | -------------------- |
| **When**     | Date, Year           |
| **Where**    | Location             |
| **In What**  | Location             |
| **How many** | Number               |

### Question Taxonomy

Researchers have tried to organize all question type, e.g.

<img src="8. Question answering.assets/image-20250505210437843.png" alt="image-20250505210437843" style="zoom:80%;" />



### However Question Taxonomies Can Get Very Large

<img src="8. Question answering.assets/image-20250505210626209.png" alt="image-20250505210626209" style="zoom:80%;" />

- Factoid Questions
  - Who, where, when, how many
  - The answers fall into a limited and somewhat predictable set of categories, see e.g. to the right
- This set is from a paper by Li and Roth, 2005, *Learning Question Classifiers*
- Their major categories include
  - Abbreviation
  - Description
  - Entity
  - Human
  - Location
  - Numeric
- Questions can be labeled at one or two levels, e.g. either as Numeric or as Numeric:date or as Location or Location:mountain
- This approach argues for a template applied to the query

### More Question Types and Examples

<img src="8. Question answering.assets/image-20250505210641038.png" alt="image-20250505210641038" style="zoom:80%;" />

### Some General Capabilities for Question - Answering Systems

- Part -of -Speech Tagging
  - a piece of software that reads text in some language and assigns parts of speech to each word , such as noun , verb , adjective , etc.
  - Markov Models are now the standard method for part - of - speech assignment
  - Some current major algorithms for part - of - speech tagging include the Viterbi algorithm , Brill tagger , and Baum - Welch algorithm , see [Part-of-speech tagging - Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging)

- Named Entity Extraction
  - Software that seeks to locate and classify named entities in text into pre - defined categories such as the names of persons , organizations , locations , expressions of times , quantities...
  - See [Named-entity recognition - Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)
- Determining Semantic Relations
  - **semantic relations are** concepts or meanings between entities . E.g. [ School ] is a kind of [ educational institution ]
    - Opportunity to use WordNet , [WordNet - Princeton University](https://wordnet.princeton.edu/)
- Dictionaries / Thesauri



### Question Processing Tool Named Entity Recognizer Example

<img src="8. Question answering.assets/image-20250505211223246.png" alt="image-20250505211223246" style="zoom:80%;" />

- The process of recognizing information units like names, including persons, organizations, location names, and numeric expressions including time, date, money and percent expressions from unstructured text. 
- This is an example of supervised learning as training sets are first created 
- See https://nlp.stanford.edu/software/ for a Java program and explanation



### Example of NLP Extraction Used to Build a Knowledge Graph:

<img src="8. Question answering.assets/image-20250505211245540.png" alt="image-20250505211245540" style="zoom:80%;" />

Nouns are (usually) entities and verbs are (usually) relationships among the entities and adjectives (usually) describe the relationships

https://kgtutorial.github.io/wsdm-slides/Part2_knowledge-extraction.pdf 

### General Keyword Selection Algorithm

1. Use the part-of-speech recognizer to identify all
   - nouns
   - verbs
   - non-stopwords in quotations
   - NNP (proper noun) words in recognized named entities
   - complex nominals with their adjectival modifiers
2. Select the answer type word

### Expanding the Keyword Set Using Variants

- There are 3 distinct ways to expand the keyword set determined by the keyword selection algorithm 
- Morphological variants (having to do with inflections) 
  - invented → inventor → inventions 
- Lexical variants (similar to synonyms) 
  - killer → assassin 
  - far → distance 
- Semantic variants 
  - like → prefer (a more specific or less specific term)

### Use WordNet for Type Identification

We have already seen the use of WordNet, a lexical database of English nouns, verbs, adjectives, adverbs 

<img src="8. Question answering.assets/image-20250505211452927.png" alt="image-20250505211452927" style="zoom:80%;" />

WordNet permits refinement of poet to specific instances 

WordNet also helps determine the TYPE of answer we are looking for

### Answer Type Taxonomy

- Use WordNet to merge named entities with the WordNet hierarchy

<img src="8. Question answering.assets/image-20250505211526705.png" alt="image-20250505211526705" style="zoom:80%;" />

Recall: WordNet is a database of English nouns, verbs, adjectives and adverbs grouped into sets of synonyms (synsets) and relations showing super/subordinate relations (hyperonymy/hyponymy) 

If you know the answer should be a person WordNet helps determine what sort of person



### Part 2: Passage Retrieval

- Once we have formulated queries using tools like NER, POS, variant expansion and WordNet results 
- Send queries to a search engine and retrieve snippet results 
- Filter the results for correct type 
  - use answer type classification 
  - Rank passages based on a trained classifier (application of machine learning) 
    - Features: 
      - Question keywords, Named Entities 
      - Longest overlapping sequence, 
      - Shortest keyword-covering span 
      - N-gram overlap between question and passage



### What is BERT

- Bidirectional Encoder Representations from Transformers 
- In 2018, Google introduced and open-sourced BERT 
  - BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context 
  - It achieved strong results on problems such as sentiment analysis, semantic role labeling, sentence classification and the disambiguation of polysemous words, or words with multiple meanings 
- In October 2019, Google announced that they would begin applying BERT to their United States based production search algorithms. 
- BERT was pre-trained using only an unlabeled, plain text corpus (namely the entirety of the English Wikipedia. 
- It continues to learn unsupervised from the unlabeled text and improve even as its being used in practical applications 
- the BERT Github repository is available on github, git clone https://github.com/google-research/bert.git  



### What is BERT used for

- Sequence-to-sequence based language generation tasks such as: 
  - Question answering 
  - Abstract summarization 
  - Sentence prediction 
  - Conversational response generation 
- Natural language understanding tasks such as: 
  - Polysemy and Coreference (words that sound or look the same but have different meanings) resolution 
  - Word sense disambiguation 
  - Natural language inference 
  - Sentiment classification 
- https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model 
- https://www.google.com/search?q=https://csci572.com/papers/BERT.pdf (the original paper)

### BERT Has Many Pre-Trained Models

- BERT encoders have larger feedforward networks (768 and 1024 nodes in Base and Large respectively) and more attention heads (12 and 16 respectively). 

- BERT was trained on Wikipedia and other datasets 

- To the right you can see a diagram of additional variants of BERT pre-trained on specialized corpora

  <img src="8. Question answering.assets/image-20250505212359214.png" alt="image-20250505212359214" style="zoom:80%;" />

### How BERT works

- BERT functions by reading bidirectionally, accounting for the effect of all other words in a sentence on the focus word and eliminating the left-to-right momentum that biases words towards a certain meaning as a sentence progresses. 
- At the right BERT is determining which prior word in the sentence the word "it" is referring to, and then using its attention mechanism to weigh the options. The word with the highest calculated score is deemed the correct association (i.e., "it" refers to "street", not "animal"). 
- If this phrase was a search query, the results would reflect this subtler, more precise understanding the BERT reached.

<img src="8. Question answering.assets/image-20250505212428075.png" alt="image-20250505212428075" style="zoom:80%;" />



### BERT for Question-Answering

- question answering is just a prediction task 
- on receiving a question as input, the goal of the application is to identify the right answer from some corpus 
- given a question and a context paragraph, the model predicts a start and an end token from the paragraph that most likely answers the question.  

<img src="8. Question answering.assets/image-20250505212500683.png" alt="image-20250505212500683" style="zoom:80%;" />

[Here](https://bytes.usc.edu/cs572/s25-555-sear-ch/lectures/QA/docs/medium.com-BERT Explained A Complete Guide with Theory and Tutorial.pdf) is a thorough explanation of BERT.