# Clustering, classification

### What is Clustering?

- Clustering: the process of grouping a set of objects into classes of similar objects
  - Documents within a cluster should be similar.
  - Documents from different clusters should be dissimilar.

- Clustering is the most common form of *unsupervised learning Unsupervised learning* = learning from raw data, as opposed to supervised learning where a classification of examples is given a priori
- Clustering is a common and important task that finds many applications in IR and other places



### Related Searches are a Form of Clustering

- Google related searches

  <img src="9. Clustering, Classification.assets/image-20250505220750746.png" alt="image-20250505220750746" style="zoom:80%;" />

- Yahoo does some clustering via alternate queries

  <img src="9. Clustering, Classification.assets/image-20250505220815744.png" alt="image-20250505220815744" style="zoom:80%;" />

- Bing does a little better

  <img src="9. Clustering, Classification.assets/image-20250505220830105.png" alt="image-20250505220830105" style="zoom:80%;" />

### yippy.com Search Engine

- **Yippy** (formerly **Clusty**) is a metasearch engine developed by Vivísimo which emphasizes clusters of results.

  - initial screen with query "cars"

    <img src="9. Clustering, Classification.assets/image-20250505220941603.png" alt="image-20250505220941603" style="zoom:80%;" />

  - clustered results appear on the left column: e.g.
    sale
    reviews
    dealers
    rentals

    <img src="9. Clustering, Classification.assets/image-20250505220949377.png" alt="image-20250505220949377" style="zoom:80%;" />

  - multiple level clusters:
    car dealers
    trucks
    ebay

    <img src="9. Clustering, Classification.assets/image-20250505221004674.png" alt="image-20250505221004674" style="zoom:80%;" />

### Yahoo's Name Derives from Yet Another Hierarchical Officious Oracle

- Yahoo! Hierarchy isn’t clustering but is the kind of output you want from clustering – a taxonomy

<img src="9. Clustering, Classification.assets/image-20250505221024954.png" alt="image-20250505221024954" style="zoom:80%;" />



### **What Is A Good Clustering?**

- **Internal criterion: A good clustering will produce high quality clusters in which:**
  - the **intra-class** (that is, intra-cluster) similarity is high
  - the **inter-class** similarity is low
  - The measured quality of a clustering depends on both the document representation and the similarity measure used

### **Three Criteria of Adequacy for Clustering Methods**

1. The method produces a clustering which is **unlikely to be altered drastically** when further objects are incorporated
   - i.e. it is stable even under significant growth
2. The method is **stable** in the sense that small errors in the description of objects lead to small changes in the clustering
3. The method is **independent** of the initial ordering of the objects



### Begin with Clustering

- Step 1: Given a large set of computer science documents, first we cluster them using some algorithm (to be presented) 

  <img src="9. Clustering, Classification.assets/image-20250505221523494.png" alt="image-20250505221523494" style="zoom:80%;" />

### Then We Name the Clusters

- Step 2: we label the clusters 
  - choosing a popular name from each document cluster 

<img src="9. Clustering, Classification.assets/image-20250505221540602.png" alt="image-20250505221540602" style="zoom:80%;" />

### Still Clustering: Determine Decision Boundaries

- Step 3: we compute boundaries for the clusters that can be used as new documents appear; i.e. classification 

  <img src="9. Clustering, Classification.assets/image-20250505221613481.png" alt="image-20250505221613481" style="zoom:80%;" />



### Classification is Different from Clustering

- In general, in **classification** you have a set of predefined classes and want to know which class a new object belongs to. 
- **Clustering** tries to group a set of objects and find whether there is some relationship between the objects. 
  - Clustering precedes classification 
- In the context of machine learning, classification is *supervised learning* and clustering is *unsupervised learning* 
  - **Clustering** requires a. an algorithm, b. a similarity measure, and c. a number of clusters 
  - **classification** has each document labeled in a class and an algorithm that assigns new documents to one of the classes

### Classification Requires Initial Clusters and Boundaries

- Definition: *Supervised Learning*, inferring a function from labeled training data

1. The documents in each cluster define the “training” docs for each category
   - E.g in computer science named clusters would include: Algorithms, Theory, AI, Databases, Operating Systems, NLP, etc.
2. Documents are in a cluster based upon the similarity measure used;
   - generally a vector space with each doc viewed as a bag of words
3. A classifier is an algorithm that will classify new docs
   - Essentially, the decision space is partitioned and an algorithm is devised
4. Given a new doc, the new algorithm determines which partition it falls into



### Now Let’s Return to the Earlier Problem: Clustering

- Questions to consider when clustering 
  - How do we represent the document? 
    - Usually as a vector space 
  - How do we compute similarity/distance? 
    - Using cosine similarity 
  - How many clusters? 
    - will it be a fixed a priori number? or 
    - completely data driven? 
  - Be careful to avoid “trivial” clusters - too large or small 
    - If a cluster is too large, then for navigation purposes you’ve wasted an extra user click without whittling down the set of documents much

### Issue: Hard vs. Soft Clustering

- **Hard clustering:** Each document belongs to exactly one cluster 
  - More common and easier to do 
- **Soft clustering:** A document can belong to more than one cluster. 
  - Makes sense for some applications e.g. news about Los Angeles might be included in local and national news clusters 
  - E.g. you may want to put a pair of sneakers in two clusters: (i) sports apparel and (ii) shoes

### What Definition of Similarity/Distance Will Be Used

- Once again we will treat documents as vectors 

  - Cosine similarity (seen before many times) 

    - Cosine similarity is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them. Range from 0 (dissimilar) to 1 (exactly similar)

      $$
      \text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = 
      \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} (A_i)^2} \cdot \sqrt{\sum_{i=1}^{n} (B_i)^2}}
      $$

    - Most clustering implementations use cosine similarity 
    
    - Euclidean distance is a close alternative that is also popular



### A Data Set with Clear Cluster Structure

<img src="9. Clustering, Classification.assets/image-20250505222049828.png" alt="image-20250505222049828" style="zoom:80%;" />

Circles represent documents as N-vectors 

- How would you design an algorithm for finding the three clusters in this case? 
- Hint: use a distance measure



### Clustering Algorithms

- Two general methodologies 
  - Partitioning Based Algorithms 
  - Hierarchical Algorithms 
- Partitioning Based 
  - Choose K and then divide a set of N items into K clusters 
- Hierarchical – Bottom Up/Top Down 
  - **agglomerative**: pairs of items or clusters are successively linked to produce larger clusters (hierarchy produced bottom-up) 
  - **divisive**: start with the whole set as a cluster and successively divide sets into smaller partitions (hierarchy produced top-down) Copyright Ellis Horowitz, 2011-2022 23



### A Partitioning Algorithm: K-Means Clustering Algorithm

- Clustering algorithm strategy
  - Choose k random data items out of the n items; call these items the means; they designate the prototype or name of the cluster
- **Refine it iteratively**
  - Associate each of the n-k items with one of the **k clusters** choosing the **cluster** that it is nearest to;
  - **This is called K-means clustering**

- Recall
  - The "mean" is the "average" where you add up all the numbers and then divide by the number of numbers.
  - The "median" is the "middle" value in the list of numbers. To find the median, you may have to sort
  - The "mode" is the value that occurs most often. If no number is repeated, then there is no mode for the list

### Different Ways of Clustering the Same Set of Points

(a) Original points. 

<img src="9. Clustering, Classification.assets/image-20250505222432236.png" alt="image-20250505222432236" style="zoom:80%;" />

(b) Two clusters.

<img src="9. Clustering, Classification.assets/image-20250505222442657.png" alt="image-20250505222442657" style="zoom:80%;" />

(c) Four clusters. [Image shows the same 10 points grouped into four clusters, using symbols: plus signs, stars, diamonds, and squares]

<img src="9. Clustering, Classification.assets/image-20250505222450958.png" alt="image-20250505222450958" style="zoom:80%;" />

(d) Six clusters. 

<img src="9. Clustering, Classification.assets/image-20250505222456856.png" alt="image-20250505222456856" style="zoom:80%;" />

K-means clustering critically depends upon the value of k

### An Approximation Clustering Algorithm

1. Select K points as initial centroids
2. repeat
   - form K clusters by assigning each remaining point to its closest centroid
   - re-compute the centroid of each cluster
3. until centroids do not change or M iterations reached 
   - the algorithm will always terminate, however it does not always find the optimal solution 
   - this is an example of a greedy algorithm

### K-Means Depends on Centroids

- Assumes instances are real-valued vectors
  - Let $\overrightarrow x$ represent the vectors in a cluster c

- Then we define the centroids, (or center of gravity), of the cluster to be the mean of the vectors in the cluster; we write this in the following way
  $$
  \vec{\mu}(c) = \frac{1}{|c|} \sum_{\vec{x} \in c} \vec{x}
  $$

- Reassignment of instances to clusters is based on distance to the current cluster centroids.

### There are Several Possible Distance Metrics

- Euclidean distance (L$_2$ norm):
  $$
  L_2(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^{m} (x_i - y_i)^2}
  $$
  
- L$_1$ norm:
  $$
  L_1(\vec{x}, \vec{y}) = \sum_{i=1}^{m} |x_i - y_i|
  $$

- Cosine Similarity (transform to a distance by subtracting from 1):
  $$
  1 - \frac{\vec{x} \cdot \vec{y}}{|\vec{x}| \cdot |\vec{y}|}
  $$

### K-means Clustering – Summary Details

- Initial centroids are often chosen randomly
  - Clusters produced vary from one run to another
- The centroid is (typically) the mean of the points in the cluster
- ‘Closeness’ is measured by cosine similarity, a variation of Euclidean distance
- Most of the convergence happens in the first few iterations.
  - Often the stopping condition is changed to ‘Until relatively few points change clusters
- Complexity is O(i∗k∗n∗m)
  - n = number of points, k = number of clusters, i = number of iterations, m = number of attributes

###### [Here](https://bytes.usc.edu/~saty/tools/xem/run.html?x=kmeans) is a demo of k-means clustering; [this](https://bytes.usc.edu/cs572/s25-555-sear-ch/lectures/clu-cla/code/k-means.html) is a copy where you can modify the code (eg. alter the number of clusters, data points, etc.).

### Hierarchical Clustering Algorithms

- Two main types of hierarchical clustering
  - Agglomerative:
    - Start with the points as individual clusters
    - At each step, merge the closest pair of clusters until only one cluster (or k clusters) left (bottom-up)
  - Divisive:
    - Start with one, all-inclusive cluster
    - At each step, split a cluster until each cluster contains a point (or there are k clusters), (top-down)

- Traditional hierarchical algorithms use a similarity or distance matrix
  - Merge or split one cluster at a time

### A Dendrogram is Used to Display Clusters

- A **dendrogram** is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering

<img src="9. Clustering, Classification.assets/image-20250505223038920.png" alt="image-20250505223038920" style="zoom:80%;" />

second row clusters are: {a}, {b c}, {d e} {f}

third row clusters are: {a}, {b c} {d e f}

<img src="9. Clustering, Classification.assets/image-20250505223049576.png" alt="image-20250505223049576" style="zoom:80%;" />

### Hierarchical Agglomerative Clustering

- HAC starts with unclustered data and performs successive pairwise joins among items (or previous clusters) to form larger ones
  - this results in a hierarchy of clusters which can be viewed as a dendrogram
  - Dendrograms are usually drawn as shown below
  - The height of an edge can sometimes refer to the degree of similarity
  - useful in pruning search in a clustered item set, or in Browse clustering results

<img src="9. Clustering, Classification.assets/image-20250505223119426.png" alt="image-20250505223119426" style="zoom:80%;" />

### Divisive Clustering Algorithm

1. Start at the top with all documents in one cluster.
2. The cluster is split using a partitioning clustering algorithm.
   - Use the k-means clustering algorithm, which is linear in computing time whereas HAC (hierarchical agglomerative clustering) algorithms are quadratic
3. Apply the procedure recursively until each document is in its own singleton cluster 
   - Studies show that the divisive algorithms produce more accurate hierarchies than bottom up
     - Bottom-up methods make clustering decisions based on local patterns without initially taking into account the global distribution. These early decisions cannot be undone.
     - Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions. Copyright Ellis Horowitz, 2011-2022 54

Rocchio Algorithm: Basics

- The Rocchio algorithm is a method of relevance feedback 

- It was initially developed by the SMART Information Retrieval System in 1960-1964. 

- It assumes documents are represented using the vector space model 

- The algorithm uses the notions of relevant/non-relevant documents and centroids 

- Recall: the **centroid** is the center of mass (or vector average) of a set of points. 

- *Definition*: Centroid 
  $$
  \vec{\mu}(C) = \frac{1}{|C|} \sum_{d \in C} \vec{d}
  $$
  where C is a set of documents, |C| is the size of the set, and d is the normalized vector representing document d 

- Note: We have seen centroids before in the k-means algorithm

### Centroid Example

<img src="9. Clustering, Classification.assets/image-20250505223655601.png" alt="image-20250505223655601" style="zoom:50%;" />

- 3 regions each with its own centroid 
- The centroid may not correspond to a document in your set 
- The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids

**Rocchio Algorithm Derivation**

Assuming someone has identified the set of relevant ($D_r$) and non-relevant ($D_{nr}$) documents, the algorithm aims to find the query $q$ that maximizes similarity with the set of relevant documents $D_r$ while minimizing similarity with the set of non-relevant documents $D_{nr}$:
$$
q_{opt} = \arg\max \left[\text{sim}(q, D_r) - \text{sim}(q, D_{nr})\right]
$$
Under cosine similarity, the optimal query for separating relevant and non-relevant documents is:
$$
q_{opt} = \frac{1}{|D_r|} \sum_{d_j \in D_r} d_j - \frac{1}{|D_{nr}|} \sum_{d_j \in D_{nr}} d_j
$$
which is the vector difference between the centroids of the relevant and non-relevant documents.

### Rocchio in Practice

- Represent query and documents as weighted vectors (e.g., tf-idf). 
- Use Rocchio formula to compute new query vector (given some known relevant / non-relevant documents). 
- Calculate cosine similarity between new query vector and the documents. 
- Rocchio has been shown useful for increasing both precision and recall because it contains aspects of positive and negative feedback. 
- Positive feedback is much more valuable than negative (i.e., indications of what *is* relevant) so typically systems set γ<β or even γ=0. 

<img src="9. Clustering, Classification.assets/image-20250505223822110.png" alt="image-20250505223822110" style="zoom:80%;" />



### 2D Rocchio Example

- For 2D examples the relevant set is generally much smaller than the non-relevant set;

- As a result we need a slightly modified rule

  <img src="9. Clustering, Classification.assets/image-20250505223907698.png" alt="image-20250505223907698" style="zoom:80%;" />

  Let circles represent relevant documents

  Let Xs represent nonrelevant documents

### 2D Rocchio Illustrated (1 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224205798.png" alt="image-20250505224205798" style="zoom:80%;" />

$\vec{\mu}R$: centroid of relevant documents

### 2D Rocchio Illustrated (2 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224250355.png" alt="image-20250505224250355" style="zoom:80%;" />

$\vec{\mu}R$ does not separate relevant / nonrelevant

### 2D Rocchio Illustrated (3 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224620995.png" alt="image-20250505224620995" style="zoom:80%;" />

$\vec{\mu}NR$ : Centroid of nonrelevant documents

### 2D Rocchio Illustrated (4 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224626841.png" alt="image-20250505224626841" style="zoom:80%;" />

Consider the two centroids

### 2D Rocchio Illustrated (5 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224633443.png" alt="image-20250505224633443" style="zoom:80%;" />

$\vec{\mu}R - \vec{\mu}NR$ : centroid difference vector

### 2D Rocchio Illustrated (6 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224641791.png" alt="image-20250505224641791" style="zoom:80%;" />

Add difference vector to $\vec{\mu}R$...

### 2D Rocchio Illustrated (7 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224649407.png" alt="image-20250505224649407" style="zoom:80%;" />

... to get $\vec q_{opt}$

### 2D Rocchio Illustrated (8 of 9)

Note that the boundary computed during the Rocchio algorithm in This case is viewed as a circle; 

Tests of new documents are easily determined to either fit within the circle or not.

<img src="9. Clustering, Classification.assets/image-20250505224658148.png" alt="image-20250505224658148" style="zoom:80%;" />

$\vec q_{opt}$ now separates relevant / nonrelevant perfectly.

### 2D Rocchio Illustrated (9 of 9)

<img src="9. Clustering, Classification.assets/image-20250505224704439.png" alt="image-20250505224704439" style="zoom:80%;" />

$\vec q_{opt}$ separates relevant / nonrelevant perfectly

### **Rocchio Algorithm Used for Classification**

- More typically, the boundary determination in Rocchio is not a circle, but a hyperplane
- Given two centroids of two classes of documents, the boundary between the two classes is the set of points with equal distance from the two centroids
- Once the boundary is determined, the Rocchio rule is to classify a point according to the region it falls into, or equivalently, determining the centroid that the point is closest to



### Classification is Different from Clustering

- In general, in **classification** you have a set of predefined classes and want to know which class a new object (document) belongs to. 
- Remember, **Clustering** tries to group a set of objects and find whether there is some relationship between the objects. 
  - we already saw two algorithms for clustering, K-Means Algorithm and Agglomerative Clustering algorithm 
- In the context of machine learning, classification is *supervised learning* and clustering is *unsupervised learning* 
  - **Clustering** requires a. an algorithm, b. a similarity measure, and c. a number of clusters 
  - **classification** has each document labeled in a class and an algorithm that assigns documents to one of the classes



### The Problem Statement for Classification

- Given two things: 
  1. A description of an instance, x∈X, where X is the *instance language* or *instance space*, and 
  2. A fixed set of categories: C={c1,c2,...,cn} 
- Determine: 
  - The category of x: c(x)∈C, where c(x) is a *categorization function* whose domain is X and whose range is C. 
  - Functions that categorize are called "classifiers"

Classification Using Vector Spaces

- In vector space classification, the training set corresponds to a labeled set of document vectors 
- Premise 1: Documents in the same class form a contiguous region of space - This is referred to as the *contiguity hypothesis* 
- Premise 2: Documents from different classes don't overlap (much) Learning a classifier is equivalent to building (defining) surfaces to delineate classes in the space

### Ways to Measure Distance

For normalized vectors Euclidean distance and cosine similarity correspond 

- Distance Functions

  - Euclidean Distance:
    $$
    \sum_{i=1}^{k} (x_i - y_i)^2
    $$

  - Manhattan Distance:
    $$
    \sum_{i=1}^{k} |x_i - y_i|
    $$

  - Minkowski Distance:
    $$
    \left( \sum_{i=1}^{k} |x_i - y_i|^q \right)^{1/q}
    $$
    



### k Nearest Neighbor Classification Algorithm

- Initially we assume we have a set of N documents that have already been classified 
  - the WDM videos assume the class names are colors (see the Schedule of Lectures) 
- To classify a document d 
  - locate among the N documents the k closest ones 
  - from these k neighbors, pick the class that occurs most often, the majority class, and use that as the label for d

### Nearest-Neighbor Without Learning

- Learning: there is no learning step; just store the labeled training examples D 
- Testing instance x (under 1-NN): 
  - Compute the distance between x and all examples in D. 
  - Assign x the category of the closest example in D. 
- Rationale of k-NN: contiguity hypothesis 
  - if documents do form contiguous regions in space, then k-NN makes sense
- Does not compute anything beyond storing the examples 
  - we are NOT determining any hierarchy or model K-NN has also been called: 
    - Case-based learning 
    - Memory-based learning 
    - Lazy learning



### k=6 (6NN)

5 neighbors are colored red, one is colored green, so the yellow diamond is colored red 

<img src="9. Clustering, Classification.assets/image-20250505225711893.png" alt="image-20250505225711893" style="zoom:80%;" />

When k=1, the document is assigned to its nearest neighbor



### K-Nearest Neighbor Another Example

- k = 1: 
  - Belongs to square class 
- k = 3: 
  - Belongs to triangle class 
- k = 7: 
  - Belongs to square class

<img src="9. Clustering, Classification.assets/image-20250505225759452.png" alt="image-20250505225759452" style="zoom:80%;" />

- Choosing the value of k: 
  - If k is too small, sensitive to noise points 
  - If k is too large, neighborhood may include points from other classes 
  - Choose an odd value for k, to eliminate ties



### Choice of K

- The best choice of k depends upon the data; 
  - generally, larger values of k reduce the effect of noise on the classification, but make boundaries between classes less distinct. 
- The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance 
  - In binary (two class) classification problems, (e.g. male or female) it is helpful to choose k to be an odd number as this avoids tied votes 
  - Choosing the optimal value for k is best done by first inspecting the data 
  - Cross-validation is another way to retrospectively determine a good value of K by using an independent dataset to validate the K value 
  - Historically, the optimal K for most datasets has been between 3-10



### Voronoi Diagram

- For the k-Nearest Neighbor Algorithm, k = 1 is a special case 

- When k = 1, each training vector defines a region in space, defining a Voronoi partition of the space 

  <img src="9. Clustering, Classification.assets/image-20250505225903613.png" alt="image-20250505225903613" style="zoom:80%;" />

### When k=1 – A Special Case

- A **Voronoi diagram** is a partitioning of a plane into regions based on distance to points in a specific subset of the plane 
- Decision boundaries in 1-NN are concatenated segments of a Voronoi tessellation (e.g. polygons) 
- The set of points (called class labels) is specified beforehand 
- For each class label there is a corresponding region consisting of all points closer to that class label than to any other. These regions are called Voronoi cells 

<img src="9. Clustering, Classification.assets/image-20250505225939374.png" alt="image-20250505225939374" style="zoom:80%;" />

20 points (class labels) and their Voroni regions; Line segments are all points equidistant to three or more regions Copyright Ellis Horowitz, 2011-2022 47

### K=1 Nearest Neighbor Regions are Polygons

- For 1-NN we assign each document to the class of its closest neighbor 

- For k-NN we assign each document to the majority class of its k closest neighbors where k is a parameter 

  <img src="9. Clustering, Classification.assets/image-20250505230009167.png" alt="image-20250505230009167" style="zoom:80%;" />

- The two classes are: X and circle, and the star document is falling into the circle area; 

  Double lines define the regions in space where documents are similar

  think of each region as defining a cellphone tower 

  K-NN is an example of a non-linear classifier; (Rocchio is a linear classifier) 

### K-NN: Final Points

- No feature selection necessary 
- No training necessary 
- Scales well with large number of classes 
  - Don’t need to train n classifiers for n classes 
- Classes can influence each other 
  - Small changes to one class can have ripple effect 
- In most cases it’s more accurate than Rocchio
